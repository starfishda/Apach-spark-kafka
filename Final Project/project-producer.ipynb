{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext, SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[*]\", appName=\"Linear Regression\")\n",
    "spark = SparkSession(sparkContext=sc)\n",
    "    \n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "#load the file into variable. make sure you read the header and schema for the table\n",
    "data = sqlcontext.read.csv('yen-au.csv', header=True, inferSchema=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect average of each collumn for missing values \n",
    "au = data.select(mean('AUD')).collect()[0]\n",
    "jp = data.select(mean('JPY')).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- AUD: double (nullable = false)\n",
      " |-- JPY: double (nullable = false)\n",
      "\n",
      "+---------+------+------+\n",
      "|     Date|   AUD|   JPY|\n",
      "+---------+------+------+\n",
      "| 1/2/1995|0.7683|107.97|\n",
      "| 1/3/1995|0.7683|107.97|\n",
      "| 1/4/1995|0.7704|100.98|\n",
      "| 1/5/1995|0.7693| 101.0|\n",
      "| 1/6/1995|0.7699|100.95|\n",
      "| 1/9/1995|0.7658|101.05|\n",
      "|1/10/1995|0.7643|100.18|\n",
      "|1/11/1995| 0.767| 99.85|\n",
      "|1/12/1995|0.7706| 100.0|\n",
      "|1/13/1995|0.7613| 98.85|\n",
      "|1/16/1995|0.7593|107.97|\n",
      "|1/17/1995|0.7616|  99.0|\n",
      "|1/18/1995|0.7596| 98.92|\n",
      "|1/19/1995|0.7613| 99.45|\n",
      "|1/20/1995|0.7674| 99.15|\n",
      "|1/23/1995|0.7701|  99.9|\n",
      "|1/24/1995|0.7698| 99.75|\n",
      "|1/25/1995|0.7711|  99.6|\n",
      "|1/26/1995|0.7683| 99.53|\n",
      "|1/27/1995|0.7625|  99.3|\n",
      "+---------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill missing value\n",
    "data = data.fillna({'AUD': round(au[0],4)})\n",
    "data = data.fillna({'JPY': round(jp[0],2)})\n",
    "\n",
    "#display the schema for the data\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                         value_serializer=lambda x:\n",
    "                         dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "\n",
    "df_index = data.select(\"*\").withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+-----+\n",
      "|     Date|   AUD|   JPY|index|\n",
      "+---------+------+------+-----+\n",
      "| 1/2/1995|0.7683|107.97|    0|\n",
      "| 1/3/1995|0.7683|107.97|    1|\n",
      "| 1/4/1995|0.7704|100.98|    2|\n",
      "| 1/5/1995|0.7693| 101.0|    3|\n",
      "| 1/6/1995|0.7699|100.95|    4|\n",
      "| 1/9/1995|0.7658|101.05|    5|\n",
      "|1/10/1995|0.7643|100.18|    6|\n",
      "|1/11/1995| 0.767| 99.85|    7|\n",
      "|1/12/1995|0.7706| 100.0|    8|\n",
      "|1/13/1995|0.7613| 98.85|    9|\n",
      "|1/16/1995|0.7593|107.97|   10|\n",
      "|1/17/1995|0.7616|  99.0|   11|\n",
      "|1/18/1995|0.7596| 98.92|   12|\n",
      "|1/19/1995|0.7613| 99.45|   13|\n",
      "|1/20/1995|0.7674| 99.15|   14|\n",
      "|1/23/1995|0.7701|  99.9|   15|\n",
      "|1/24/1995|0.7698| 99.75|   16|\n",
      "|1/25/1995|0.7711|  99.6|   17|\n",
      "|1/26/1995|0.7683| 99.53|   18|\n",
      "|1/27/1995|0.7625|  99.3|   19|\n",
      "+---------+------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_index.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4b94f5d0226c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       str(df_index.select(\"JPY\").filter(df_index.index == i).collect()[0][0])}\n\u001b[1;32m      6\u001b[0m     \u001b[0mproducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfTest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(df_index.count()-1):\n",
    "    #data = {'row': df_index.select('last_name').filter(df_index.index == 0)}\n",
    "    data = {'row': str(df_index.select(\"Date\").filter(df_index.index == i).collect()[0][0]) + \\\n",
    "            \" \"+ str(df_index.select(\"AUD\").filter(df_index.index == i).collect()[0][0]) +\" \"+ \\\n",
    "                      str(df_index.select(\"JPY\").filter(df_index.index == i).collect()[0][0])}\n",
    "    producer.send('dfTest', value=data)\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
