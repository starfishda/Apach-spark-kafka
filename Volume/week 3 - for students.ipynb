{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name : < Kwak Su Bin >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access using SparkSQL and Dataframe\n",
    "\n",
    "## Activity : Join Queries\n",
    "\n",
    "In this module, you will practice how to write codes to retrieve data using Spark SQL and Dataframes API.\n",
    "\n",
    "The complete list of Dataframe functions can be accessed from [here](https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html), [here](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) and [here](https://spark.apache.org/docs/2.2.0/api/scala/index.html#org.apache.spark.sql.functions$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we will use HR schema as shown below\n",
    "![hr](HR.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION\n",
    "The first section of this scipt is the initialization section. \n",
    "In this section, we are preparing Spark environment to recognize and process SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#additional \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# local[*]: run Spark locally with as many working processors as logical cores on your machine.\n",
    "# In the field of `master`, we use a local server with as many working processors (or threads) as possible (i.e. `local[*]`). \n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as `local[k]`.\n",
    "# The `appName` field is a name to be shown on the Sparking cluster UI. \n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(master=\"local[3]\", appName=\"Week 2 - Join Query\")\n",
    "spark = SparkSession(sparkContext=sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURE DEFINITION\n",
    "\n",
    "In this section, we are preparing the data structure to match the datafiles provided as the datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES TABLE\n",
    "scCountries = StructType([StructField(\"country_id\",StringType()),StructField(\"country_name\",StringType()),StructField(\"region_id\",IntegerType())])\n",
    "\n",
    "#DEPARTMENTS TABLE\n",
    "scDepartments = StructType([StructField(\"department_id\",IntegerType()),\n",
    "StructField(\"department_name\",StringType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"location_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#EMPLOYEES TABLE\n",
    "scEmployees = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"first_name\",StringType()),\n",
    "StructField(\"last_name\",StringType()),\n",
    "StructField(\"email\",StringType()),\n",
    "StructField(\"phone_number\",StringType()),\n",
    "StructField(\"hire_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"salary\",IntegerType()),\n",
    "StructField(\"commission_pct\",FloatType()),\n",
    "StructField(\"manager_id\",IntegerType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOBS TABLE\n",
    "scJobs = StructType([\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"job_title\",StringType()),\n",
    "StructField(\"min_salary\",IntegerType()),\n",
    "StructField(\"max_salary\",IntegerType())\n",
    "])\n",
    "\n",
    "#JOB_HISTORY TABLE\n",
    "scJob_history = StructType([\n",
    "StructField(\"employee_id\",IntegerType()),\n",
    "StructField(\"start_date\",StringType()),\n",
    "StructField(\"end_date\",StringType()),\n",
    "StructField(\"job_id\",StringType()),\n",
    "StructField(\"department_id\",IntegerType())\n",
    "])\n",
    "\n",
    "#LOCATIONS TABLE\n",
    "scLocations = StructType([\n",
    "StructField(\"location_id\",IntegerType()),\n",
    "StructField(\"street_address\",StringType()),\n",
    "StructField(\"postal_code\",StringType()),\n",
    "StructField(\"city\",StringType()),\n",
    "StructField(\"state_province\",StringType()),\n",
    "StructField(\"country_id\",StringType())\n",
    "])\n",
    "\n",
    "#REGIONS TABLE\n",
    "scRegions = StructType([\n",
    "StructField(\"region_id\",IntegerType()),\n",
    "StructField(\"region_name\",StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTRIES DATA\n",
    "dataCountries = sc.textFile('COUNTRIES.csv')\n",
    "dataCountries = dataCountries.map(lambda x: x.split(','))\n",
    "dataCountries = dataCountries.map(lambda x: [x[0],x[1], int(x[2])])\n",
    "\n",
    "#DEPARTMENTS DATA\n",
    "dataDepartments = sc.textFile('DEPARTMENTS.csv')\n",
    "dataDepartments = dataDepartments.map(lambda x: x.split(','))\n",
    "dataDepartments = dataDepartments.map(lambda x: [int(x[0]),x[1], int(x[2]), int(x[3])])\n",
    "\n",
    "#EMPLOYEES DATA\n",
    "dataEmployees = sc.textFile('EMPLOYEES.csv')\n",
    "dataEmployees = dataEmployees.map(lambda x: x.split(','))\n",
    "dataEmployees = dataEmployees.map(lambda x: [int(x[0]),x[1], x[2], \\\n",
    "                                             x[3],x[4], x[5], x[6], \\\n",
    "                                             int(x[7]),float(x[8]), int(x[9]), int(x[10])\\\n",
    "                                            ])\n",
    "\n",
    "#JOBS_DATA\n",
    "dataJobs = sc.textFile('JOBS.csv')\n",
    "dataJobs = dataJobs.map(lambda x: x.split(','))\n",
    "dataJobs = dataJobs.map(lambda x: [x[0],x[1], \\\n",
    "                                   int(x[2]),int(x[3])\\\n",
    "                                   ])\n",
    "\n",
    "#JOB_HISTORY_DATA\n",
    "dataJob_history = sc.textFile('JOB_HISTORY.csv')\n",
    "dataJob_history = dataJob_history.map(lambda x: x.split(','))\n",
    "dataJob_history = dataJob_history.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],int(x[4])\\\n",
    "                                   ])\n",
    "\n",
    "#LOCATION_DATA\n",
    "dataLocations = sc.textFile('LOCATIONS.csv')\n",
    "dataLocations = dataLocations.map(lambda x: x.split(','))\n",
    "dataLocations = dataLocations.map(lambda x: [int(x[0]),x[1], \\\n",
    "                                   x[2],x[3],x[4],x[5]\\\n",
    "                                   ])\n",
    "#REGIONS DATA\n",
    "dataRegions = sc.textFile('REGIONS.csv')\n",
    "dataRegions = dataRegions.map(lambda x: x.split(','))\n",
    "dataRegions = dataRegions.map(lambda x: [int(x[0]),x[1] ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARING DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCountries = spark.createDataFrame(dataCountries,schema=scCountries) \n",
    "dfCountries.createOrReplaceTempView(\"dataCountries\")\n",
    "\n",
    "dfDepartments = spark.createDataFrame(dataDepartments,schema=scDepartments) \n",
    "dfDepartments.createOrReplaceTempView(\"dataDepartments\")\n",
    "\n",
    "dfEmployees = spark.createDataFrame(dataEmployees,schema=scEmployees) \n",
    "dfEmployees.createOrReplaceTempView(\"dataEmployees\")\n",
    "\n",
    "dfJobs = spark.createDataFrame(dataJobs,schema=scJobs) \n",
    "dfJobs.createOrReplaceTempView(\"dataJobs\")\n",
    "\n",
    "dfJob_history = spark.createDataFrame(dataJob_history,schema=scJob_history) \n",
    "dfJob_history.createOrReplaceTempView(\"dataJob_history\")\n",
    "\n",
    "dfLocations = spark.createDataFrame(dataLocations,schema=scLocations) \n",
    "dfLocations.createOrReplaceTempView(\"dataLocations\")\n",
    "\n",
    "dfRegions = spark.createDataFrame(dataRegions,schema=scRegions) \n",
    "dfRegions.createOrReplaceTempView(\"dataRegions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join & Outer Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Display employees' full name with department name \n",
    "\n",
    "Ensure you have the same format as expected output below \n",
    "\n",
    "![picture](lab3_q1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|         FullName|department_name|\n",
      "+-----------------+---------------+\n",
      "|Michael Hartstein|      Marketing|\n",
      "|          Pat Fay|      Marketing|\n",
      "|     Susan Mavris|Human Resources|\n",
      "|  Nancy Greenberg|        Finance|\n",
      "|    Daniel Faviet|        Finance|\n",
      "|        John Chen|        Finance|\n",
      "|   Ismael Sciarra|        Finance|\n",
      "|Jose Manuel Urman|        Finance|\n",
      "|        Luis Popp|        Finance|\n",
      "|  Jennifer Whalen| Administration|\n",
      "|    Matthew Weiss|       Shipping|\n",
      "|       Adam Fripp|       Shipping|\n",
      "|   Payam Kaufling|       Shipping|\n",
      "|   Shanta Vollman|       Shipping|\n",
      "|    Kevin Mourgos|       Shipping|\n",
      "|      Julia Nayer|       Shipping|\n",
      "|Irene Mikkilineni|       Shipping|\n",
      "|     James Landry|       Shipping|\n",
      "|    Steven Markle|       Shipping|\n",
      "|     Laura Bissot|       Shipping|\n",
      "+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "spark.sql(\"select dataEmployees.first_name || ' ' || dataEmployees.last_name AS FullName, dataDepartments.department_name from dataEmployees join dataDepartments on dataEmployees.department_id = dataDepartments.department_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+\n",
      "|         FullName|department_name|\n",
      "+-----------------+---------------+\n",
      "|Michael Hartstein|      Marketing|\n",
      "|          Pat Fay|      Marketing|\n",
      "|     Susan Mavris|Human Resources|\n",
      "|  Nancy Greenberg|        Finance|\n",
      "|    Daniel Faviet|        Finance|\n",
      "|        John Chen|        Finance|\n",
      "|   Ismael Sciarra|        Finance|\n",
      "|Jose Manuel Urman|        Finance|\n",
      "|        Luis Popp|        Finance|\n",
      "|  Jennifer Whalen| Administration|\n",
      "|    Matthew Weiss|       Shipping|\n",
      "|       Adam Fripp|       Shipping|\n",
      "|   Payam Kaufling|       Shipping|\n",
      "|   Shanta Vollman|       Shipping|\n",
      "|    Kevin Mourgos|       Shipping|\n",
      "|      Julia Nayer|       Shipping|\n",
      "|Irene Mikkilineni|       Shipping|\n",
      "|     James Landry|       Shipping|\n",
      "|    Steven Markle|       Shipping|\n",
      "|     Laura Bissot|       Shipping|\n",
      "+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame functions\n",
    "dfEmployees.join(dfDepartments, dfEmployees.department_id == dfDepartments.department_id).select(concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias('FullName'), \"department_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "Using a Self-Join concept, display employee's full name, job id and manager's name\n",
    "\n",
    "![figure](lab3_q2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------------+\n",
      "|         FullName|    job_id|     ManagerName|\n",
      "+-----------------+----------+----------------+\n",
      "|        Lisa Ozer|    SA_REP|Gerald Cambrault|\n",
      "|   Harrison Bloom|    SA_REP|Gerald Cambrault|\n",
      "|       Tayler Fox|    SA_REP|Gerald Cambrault|\n",
      "|    William Smith|    SA_REP|Gerald Cambrault|\n",
      "|  Elizabeth Bates|    SA_REP|Gerald Cambrault|\n",
      "|    Sundita Kumar|    SA_REP|Gerald Cambrault|\n",
      "|    Daniel Faviet|FI_ACCOUNT| Nancy Greenberg|\n",
      "|        John Chen|FI_ACCOUNT| Nancy Greenberg|\n",
      "|   Ismael Sciarra|FI_ACCOUNT| Nancy Greenberg|\n",
      "|Jose Manuel Urman|FI_ACCOUNT| Nancy Greenberg|\n",
      "|        Luis Popp|FI_ACCOUNT| Nancy Greenberg|\n",
      "|  Nancy Greenberg|    FI_MGR|   Neena Kochhar|\n",
      "|  Jennifer Whalen|   AD_ASST|   Neena Kochhar|\n",
      "|     Susan Mavris|    HR_REP|   Neena Kochhar|\n",
      "|     Hermann Baer|    PR_REP|   Neena Kochhar|\n",
      "|  Shelley Higgins|    AC_MGR|   Neena Kochhar|\n",
      "|      Bruce Ernst|   IT_PROG|Alexander Hunold|\n",
      "|     David Austin|   IT_PROG|Alexander Hunold|\n",
      "|  Valli Pataballa|   IT_PROG|Alexander Hunold|\n",
      "|    Diana Lorentz|   IT_PROG|Alexander Hunold|\n",
      "+-----------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "spark.sql(\"select e.first_name || ' ' || e.last_name as FullName, e.job_id, m.first_name || ' ' || m.last_name as ManagerName from dataEmployees e join dataEmployees m on m.employee_id = e.manager_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------------+\n",
      "|         FullName|    job_id|     ManagerName|\n",
      "+-----------------+----------+----------------+\n",
      "|        Lisa Ozer|    SA_REP|Gerald Cambrault|\n",
      "|   Harrison Bloom|    SA_REP|Gerald Cambrault|\n",
      "|       Tayler Fox|    SA_REP|Gerald Cambrault|\n",
      "|    William Smith|    SA_REP|Gerald Cambrault|\n",
      "|  Elizabeth Bates|    SA_REP|Gerald Cambrault|\n",
      "|    Sundita Kumar|    SA_REP|Gerald Cambrault|\n",
      "|    Daniel Faviet|FI_ACCOUNT| Nancy Greenberg|\n",
      "|        John Chen|FI_ACCOUNT| Nancy Greenberg|\n",
      "|   Ismael Sciarra|FI_ACCOUNT| Nancy Greenberg|\n",
      "|Jose Manuel Urman|FI_ACCOUNT| Nancy Greenberg|\n",
      "|        Luis Popp|FI_ACCOUNT| Nancy Greenberg|\n",
      "|  Nancy Greenberg|    FI_MGR|   Neena Kochhar|\n",
      "|  Jennifer Whalen|   AD_ASST|   Neena Kochhar|\n",
      "|     Susan Mavris|    HR_REP|   Neena Kochhar|\n",
      "|     Hermann Baer|    PR_REP|   Neena Kochhar|\n",
      "|  Shelley Higgins|    AC_MGR|   Neena Kochhar|\n",
      "|      Bruce Ernst|   IT_PROG|Alexander Hunold|\n",
      "|     David Austin|   IT_PROG|Alexander Hunold|\n",
      "|  Valli Pataballa|   IT_PROG|Alexander Hunold|\n",
      "|    Diana Lorentz|   IT_PROG|Alexander Hunold|\n",
      "+-----------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "dfEmployees.alias('df').join(dfEmployees.alias('df2')).where('df.employee_id = df2.manager_id').select(concat(col(\"df2.first_name\"), lit(\" \"), col(\"df2.last_name\")).alias('FullName'), 'df2.job_id',concat(col(\"df.first_name\"), lit(\" \"), col(\"df.last_name\")).alias('ManagerName')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "Display all departments that have no employees in it.\n",
    "\n",
    "![figure](lab3_q3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     department_name|\n",
      "+--------------------+\n",
      "|       Corporate Tax|\n",
      "|    Government Sales|\n",
      "|             Payroll|\n",
      "|          Recruiting|\n",
      "|        Construction|\n",
      "|                 NOC|\n",
      "|            Treasury|\n",
      "|Shareholder Services|\n",
      "|        Retail Sales|\n",
      "|         Contracting|\n",
      "|          IT Support|\n",
      "|  Control And Credit|\n",
      "|         IT Helpdesk|\n",
      "|       Manufacturing|\n",
      "|          Operations|\n",
      "|            Benefits|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "spark.sql(\"select department_name from dataEmployees e FULL OUTER JOIN dataDepartments d on (e.department_id=d.department_id) group by department_name having count(employee_id)=0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     department_name|\n",
      "+--------------------+\n",
      "|       Corporate Tax|\n",
      "|    Government Sales|\n",
      "|             Payroll|\n",
      "|          Recruiting|\n",
      "|        Construction|\n",
      "|                 NOC|\n",
      "|            Treasury|\n",
      "|Shareholder Services|\n",
      "|        Retail Sales|\n",
      "|         Contracting|\n",
      "|          IT Support|\n",
      "|  Control And Credit|\n",
      "|         IT Helpdesk|\n",
      "|       Manufacturing|\n",
      "|          Operations|\n",
      "|            Benefits|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "dfEmployees.join(dfDepartments,dfEmployees.department_id==dfDepartments.department_id,'full_outer').groupBy('department_name').agg(count(\"employee_id\").alias(\"merge\")).select(\"department_name\").where('merge = 0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Find employees that have the same job with his previous job. Display their previous job titles as well.\n",
    "![figure](lab3_q4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|           job_title|\n",
      "+----------+---------+--------------------+\n",
      "|  Jonathon|   Taylor|Sales Representative|\n",
      "|  Jennifer|   Whalen|Administration As...|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "spark.sql(\"select first_name, last_name, job_title from dataEmployees join dataJobs on dataEmployees.job_id = dataJobs.job_id join dataJob_history on dataEmployees.employee_id = dataJob_history.employee_id and dataEmployees.job_id = dataJob_history.job_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|           job_title|\n",
      "+----------+---------+--------------------+\n",
      "|  Jonathon|   Taylor|Sales Representative|\n",
      "|  Jennifer|   Whalen|Administration As...|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "cond = [dfEmployees.employee_id == dfJob_history.employee_id, dfEmployees.job_id == dfJob_history.job_id]\n",
    "dfEmployees.join(dfJobs, dfEmployees.job_id == dfJobs.job_id).join(dfJob_history, cond).select(dfEmployees.first_name, dfEmployees.last_name, dfJobs.job_title).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5\n",
    "\n",
    "Employees total salary can be calculated as their base salary + percentage of commision with their salary.\n",
    "Find all employees that earn total salary more than maximum salary for his job.\n",
    "\n",
    "![figure](lab3_q5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+--------------+-------+------------+\n",
      "|first_name|job_id|salary|commission_pct|  total|       range|\n",
      "+----------+------+------+--------------+-------+------------+\n",
      "|     Peter|SA_REP| 10000|           0.3|13000.0|6000 - 12008|\n",
      "|   Janette|SA_REP| 10000|          0.35|13500.0|6000 - 12008|\n",
      "|   Patrick|SA_REP|  9500|          0.35|12825.0|6000 - 12008|\n",
      "|     Allan|SA_REP|  9000|          0.35|12150.0|6000 - 12008|\n",
      "|     Clara|SA_REP| 10500|          0.25|13125.0|6000 - 12008|\n",
      "|      Lisa|SA_REP| 11500|          0.25|14375.0|6000 - 12008|\n",
      "|     Ellen|SA_REP| 11000|           0.3|14300.0|6000 - 12008|\n",
      "+----------+------+------+--------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql()\n",
    "sqlQry = spark.sql(\"select B.first_name, B.job_id, B.salary, B.commission_pct\"+\n",
    "                   \", B.salary+(B.commission_pct*B.salary) as total, concat(A.min_salary,' - ',A.max_salary) range \"+\n",
    "                   \"from dataJobs A full join dataEmployees B on A.job_id = B.job_id \"+\n",
    "                   \"where B.salary+(B.commission_pct*B.salary) > A.max_salary\")\n",
    "sqlQry.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+--------------+-------+------------+\n",
      "|first_name|job_id|salary|commission_pct|  total|       range|\n",
      "+----------+------+------+--------------+-------+------------+\n",
      "|     Peter|SA_REP| 10000|           0.3|13000.0|6000 - 12008|\n",
      "|   Janette|SA_REP| 10000|          0.35|13500.0|6000 - 12008|\n",
      "|   Patrick|SA_REP|  9500|          0.35|12825.0|6000 - 12008|\n",
      "|     Allan|SA_REP|  9000|          0.35|12150.0|6000 - 12008|\n",
      "|     Clara|SA_REP| 10500|          0.25|13125.0|6000 - 12008|\n",
      "|      Lisa|SA_REP| 11500|          0.25|14375.0|6000 - 12008|\n",
      "|     Ellen|SA_REP| 11000|           0.3|14300.0|6000 - 12008|\n",
      "+----------+------+------+--------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Put equivalence Dataframe API script in here\n",
    "tmp = dfJobs.join(dfEmployees, dfEmployees.job_id == dfJobs.job_id)\n",
    "temp = tmp.select(dfEmployees.first_name,dfEmployees.job_id,dfEmployees.salary,dfEmployees.commission_pct,\\\n",
    "                  (dfEmployees.salary + (dfEmployees.commission_pct*dfEmployees.salary)).alias(\"total\"),\\\n",
    "                  concat(dfJobs.min_salary,lit(\" - \"), dfJobs.max_salary).alias(\"range\")).where(dfEmployees.salary+(dfEmployees.commission_pct*dfEmployees.salary) > dfJobs.max_salary)\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|    job_id|max(salary)|\n",
      "+----------+-----------+\n",
      "|FI_ACCOUNT|       9000|\n",
      "|    MK_MAN|      13000|\n",
      "|   IT_PROG|       9000|\n",
      "|    FI_MGR|      12008|\n",
      "|AC_ACCOUNT|       8300|\n",
      "|    HR_REP|       6500|\n",
      "|  PU_CLERK|       3100|\n",
      "|    AC_MGR|      12008|\n",
      "|    PR_REP|      10000|\n",
      "|    ST_MAN|       8200|\n",
      "|    MK_REP|       6000|\n",
      "|    SA_REP|      11500|\n",
      "|    SA_MAN|      14000|\n",
      "|    PU_MAN|      11000|\n",
      "|  SH_CLERK|       4200|\n",
      "|   AD_PRES|      24000|\n",
      "|  ST_CLERK|       3600|\n",
      "|   AD_ASST|       4400|\n",
      "|     AD_VP|      17000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select job_id, (max(salary)) from dataEmployees group by job_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6\n",
    "\n",
    "Using Join syntax, find all employees who haven't changed their job and were hired on January\n",
    "![figure](lab3_q6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|first_name|  job_id|hire_date|\n",
      "+----------+--------+---------+\n",
      "| Alexander| IT_PROG|03-Jan-06|\n",
      "|     Karen|  SA_MAN|05-Jan-05|\n",
      "|    Curtis|ST_CLERK|29-Jan-05|\n",
      "|    Mattea|  SA_REP|24-Jan-08|\n",
      "|   Charles|  SA_REP|04-Jan-08|\n",
      "|     James|ST_CLERK|14-Jan-07|\n",
      "|   Janette|  SA_REP|30-Jan-04|\n",
      "|    Tayler|  SA_REP|24-Jan-06|\n",
      "|     Peter|  SA_REP|30-Jan-05|\n",
      "|   Douglas|SH_CLERK|13-Jan-08|\n",
      "|     Eleni|  SA_MAN|29-Jan-08|\n",
      "|   Nandita|SH_CLERK|27-Jan-04|\n",
      "|   Winston|SH_CLERK|24-Jan-06|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql\n",
    "spark.sql(\"SELECT de.first_name, de.job_id, de.hire_date FROM dataJob_history as dh FULL JOIN dataEmployees as de \"+\n",
    "          \"ON dh.employee_id = de.employee_id WHERE hire_date LIKE '%Jan%' \"+\n",
    "          \"AND de.employee_id NOT IN (SELECT employee_id FROM dataJob_history)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------+\n",
      "|first_name|  job_id|hire_date|\n",
      "+----------+--------+---------+\n",
      "| Alexander| IT_PROG|03-Jan-06|\n",
      "|     Karen|  SA_MAN|05-Jan-05|\n",
      "|    Curtis|ST_CLERK|29-Jan-05|\n",
      "|    Mattea|  SA_REP|24-Jan-08|\n",
      "|   Charles|  SA_REP|04-Jan-08|\n",
      "|     James|ST_CLERK|14-Jan-07|\n",
      "|   Janette|  SA_REP|30-Jan-04|\n",
      "|    Tayler|  SA_REP|24-Jan-06|\n",
      "|     Peter|  SA_REP|30-Jan-05|\n",
      "|   Douglas|SH_CLERK|13-Jan-08|\n",
      "|     Eleni|  SA_MAN|29-Jan-08|\n",
      "|   Nandita|SH_CLERK|27-Jan-04|\n",
      "|   Winston|SH_CLERK|24-Jan-06|\n",
      "+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dataframe function\n",
    "dfEmployees.join(dfJobs, dfEmployees.job_id == dfJobs.job_id).join(dfJob_history, dfEmployees.employee_id == dfJob_history.employee_id , 'leftanti').select(\"first_name\", dfJobs.job_id, \"hire_date\").where(col('hire_date').like(\"%Jan%\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
